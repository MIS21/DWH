{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2cc7ad-9510-4d68-8d2d-8793f1006157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:13:20,590 - INFO - Чтение данных из таблицы source1.craft_market_wide\n",
      "2025-02-19 14:13:26,281 - INFO - Успешно прочитано 999 строк из source1.craft_market_wide\n",
      "2025-02-19 14:13:26,283 - INFO - Чтение данных из таблицы source2.craft_market_masters_products\n",
      "2025-02-19 14:13:26,701 - INFO - Успешно прочитано 999 строк из source2.craft_market_masters_products\n",
      "2025-02-19 14:13:26,703 - INFO - Чтение данных из таблицы source2.craft_market_orders_customers\n",
      "2025-02-19 14:13:27,048 - INFO - Успешно прочитано 999 строк из source2.craft_market_orders_customers\n",
      "2025-02-19 14:13:27,051 - INFO - Чтение данных из таблицы source3.craft_market_craftsmans\n",
      "2025-02-19 14:13:27,386 - INFO - Успешно прочитано 999 строк из source3.craft_market_craftsmans\n",
      "2025-02-19 14:13:27,389 - INFO - Чтение данных из таблицы source3.craft_market_customers\n",
      "2025-02-19 14:13:27,675 - INFO - Успешно прочитано 999 строк из source3.craft_market_customers\n",
      "2025-02-19 14:13:27,677 - INFO - Чтение данных из таблицы source3.craft_market_orders\n",
      "2025-02-19 14:13:27,943 - INFO - Успешно прочитано 999 строк из source3.craft_market_orders\n",
      "2025-02-19 14:13:27,948 - INFO - Заполнение таблицы d_customers\n",
      "2025-02-19 14:13:28,442 - INFO - Запись данных в таблицу dwh.d_customers\n",
      "2025-02-19 14:13:33,219 - INFO - Данные успешно записаны в dwh.d_customers\n",
      "2025-02-19 14:13:33,220 - INFO - Заполнение таблицы d_craftsmans\n",
      "2025-02-19 14:13:33,388 - INFO - Запись данных в таблицу dwh.d_craftsmans\n",
      "2025-02-19 14:13:34,802 - INFO - Данные успешно записаны в dwh.d_craftsmans\n",
      "2025-02-19 14:13:34,804 - INFO - Заполнение таблицы d_products\n",
      "2025-02-19 14:13:34,975 - INFO - Запись данных в таблицу dwh.d_products\n",
      "2025-02-19 14:13:36,859 - INFO - Данные успешно записаны в dwh.d_products\n",
      "2025-02-19 14:13:36,861 - INFO - Подготовка данных заказов из источников\n",
      "2025-02-19 14:13:37,184 - INFO - Чтение данных из таблицы dwh.d_customers\n",
      "2025-02-19 14:13:37,481 - INFO - Успешно прочитано 17982 строк из dwh.d_customers\n",
      "2025-02-19 14:13:37,517 - INFO - Чтение данных из таблицы dwh.d_craftsmans\n",
      "2025-02-19 14:13:37,801 - INFO - Успешно прочитано 17970 строк из dwh.d_craftsmans\n",
      "2025-02-19 14:13:37,819 - INFO - Чтение данных из таблицы dwh.d_products\n",
      "2025-02-19 14:13:38,036 - INFO - Успешно прочитано 17964 строк из dwh.d_products\n",
      "2025-02-19 14:13:38,179 - INFO - Заполнение таблицы фактов f_orders\n",
      "2025-02-19 14:13:38,180 - INFO - Запись данных в таблицу dwh.f_orders\n",
      "2025-02-19 14:14:36,076 - INFO - Данные успешно записаны в dwh.f_orders\n",
      "2025-02-19 14:14:36,077 - INFO - ETL-процесс успешно завершён\n",
      "2025-02-19 14:14:36,455 - INFO - SparkSession остановлен\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from functools import reduce\n",
    "import logging\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dimensions and Facts\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.7.1.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://craft_market_db:5432/craft_market\"\n",
    "connection_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Функция для чтения данных из PostgreSQL\n",
    "def read_table(table_name, schema):\n",
    "    try:\n",
    "        logger.info(f\"Чтение данных из таблицы {schema}.{table_name}\")\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        logger.info(f\"Успешно прочитано {df.count()} строк из {schema}.{table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при чтении таблицы {schema}.{table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def write_table(df, table_name, schema, columns=None):\n",
    "    try:\n",
    "        logger.info(f\"Запись данных в таблицу {schema}.{table_name}\")\n",
    "        # Если указаны столбцы, выбираем только их\n",
    "        if columns:\n",
    "            df = df.select(*columns)\n",
    "        \n",
    "        # Записываем данные в таблицу\n",
    "        df.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            mode=\"append\",\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        logger.info(f\"Данные успешно записаны в {schema}.{table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при записи в таблицу {schema}.{table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def union_all(*dfs):\n",
    "    return reduce(DataFrame.unionByName, dfs)\n",
    "\n",
    "try:\n",
    "    # Чтение данных из источников\n",
    "    source1_data = read_table(\"craft_market_wide\", \"source1\")\n",
    "    source2_masters_products = read_table(\"craft_market_masters_products\", \"source2\")\n",
    "    source2_orders_customers = read_table(\"craft_market_orders_customers\", \"source2\")\n",
    "    \n",
    "    # Для source3 читаем сразу все три таблицы\n",
    "    source3_craftsmans = read_table(\"craft_market_craftsmans\", \"source3\")\n",
    "    source3_customers = read_table(\"craft_market_customers\", \"source3\")\n",
    "    source3_orders    = read_table(\"craft_market_orders\", \"source3\")\n",
    "    \n",
    "    # 1. Загрузка измерений (dimensions)\n",
    "    # d_customers (используем customer_email как бизнес-ключ)\n",
    "    logger.info(\"Заполнение таблицы d_customers\")\n",
    "    customers_df1 = source1_data.select(\n",
    "        col(\"customer_email\"),\n",
    "        col(\"customer_name\"),\n",
    "        col(\"customer_address\"),\n",
    "        col(\"customer_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    customers_df2 = source2_orders_customers.select(\n",
    "        col(\"customer_email\"),\n",
    "        col(\"customer_name\"),\n",
    "        col(\"customer_address\"),\n",
    "        col(\"customer_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    customers_df3 = source3_customers.select(\n",
    "        col(\"customer_email\"),\n",
    "        col(\"customer_name\"),\n",
    "        col(\"customer_address\"),\n",
    "        col(\"customer_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    customers_df_final = union_all(customers_df1, customers_df2, customers_df3).distinct()\\\n",
    "        .withColumn(\"load_dttm\", current_timestamp())\n",
    "    write_table(customers_df_final, \"d_customers\", \"dwh\", \n",
    "                columns=[\"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\", \"load_dttm\"])\n",
    "    \n",
    "    # d_craftsmans (используем craftsman_email как бизнес-ключ)\n",
    "    logger.info(\"Заполнение таблицы d_craftsmans\")\n",
    "    craftsmans_df1 = source1_data.select(\n",
    "        col(\"craftsman_email\"),\n",
    "        col(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    craftsmans_df2 = source2_masters_products.select(\n",
    "        col(\"craftsman_email\"),\n",
    "        col(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    craftsmans_df3 = source3_craftsmans.select(\n",
    "        col(\"craftsman_email\"),\n",
    "        col(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").cast(\"date\")\n",
    "    ).distinct()\n",
    "\n",
    "    craftsmans_df_final = union_all(craftsmans_df1, craftsmans_df2, craftsmans_df3).distinct()\\\n",
    "        .withColumn(\"load_dttm\", current_timestamp())\n",
    "    write_table(craftsmans_df_final, \"d_craftsmans\", \"dwh\", \n",
    "                columns=[\"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\", \"load_dttm\"])\n",
    "    \n",
    "    # d_products (используем product_name как бизнес-ключ)\n",
    "    logger.info(\"Заполнение таблицы d_products\")\n",
    "    products_df1 = source1_data.select(\n",
    "        col(\"product_name\"),\n",
    "        col(\"product_description\"),\n",
    "        col(\"product_type\"),\n",
    "        col(\"product_price\").cast(\"bigint\")\n",
    "    ).distinct()\n",
    "\n",
    "    products_df2 = source2_masters_products.select(\n",
    "        col(\"product_name\"),\n",
    "        col(\"product_description\"),\n",
    "        col(\"product_type\"),\n",
    "        col(\"product_price\").cast(\"bigint\")\n",
    "    ).distinct()\n",
    "\n",
    "    products_df3 = source3_orders.select(\n",
    "        col(\"product_name\"),\n",
    "        col(\"product_description\"),\n",
    "        col(\"product_type\"),\n",
    "        col(\"product_price\").cast(\"bigint\")\n",
    "    ).distinct()\n",
    "\n",
    "    products_df_final = union_all(products_df1, products_df2, products_df3).distinct()\\\n",
    "        .withColumn(\"load_dttm\", current_timestamp())\n",
    "    write_table(products_df_final, \"d_products\", \"dwh\", \n",
    "                columns=[\"product_name\", \"product_description\", \"product_type\", \"product_price\", \"load_dttm\"])\n",
    "\n",
    "\n",
    "    # 2. Подготовка данных для фактов (fact orders)\n",
    "    logger.info(\"Подготовка данных заказов из источников\")\n",
    "    orders_df1 = source1_data.select(\n",
    "        col(\"order_id\").cast(\"bigint\"),\n",
    "        col(\"order_created_date\").cast(\"date\"),\n",
    "        col(\"order_completion_date\").cast(\"date\"),\n",
    "        col(\"order_status\"),\n",
    "        col(\"customer_email\"),\n",
    "        col(\"craftsman_email\"),\n",
    "        col(\"product_name\")\n",
    "    )\n",
    "    \n",
    "    orders_df2 = source2_orders_customers.alias(\"oc\")\\\n",
    "        .join(source2_masters_products.alias(\"mp\"), col(\"oc.craftsman_id\") == col(\"mp.craftsman_id\"))\\\n",
    "        .select(\n",
    "            col(\"oc.order_id\").cast(\"bigint\"),\n",
    "            col(\"oc.order_created_date\").cast(\"date\"),\n",
    "            col(\"oc.order_completion_date\").cast(\"date\"),\n",
    "            col(\"oc.order_status\"),\n",
    "            col(\"oc.customer_email\"),\n",
    "            col(\"mp.craftsman_email\"),\n",
    "            col(\"mp.product_name\")\n",
    "        )\n",
    "    \n",
    "    orders_df3 = source3_orders.alias(\"o\")\\\n",
    "        .join(source3_craftsmans.alias(\"csm\"), col(\"o.craftsman_id\") == col(\"csm.craftsman_id\"))\\\n",
    "        .join(source3_customers.alias(\"cs\"), col(\"o.customer_id\") == col(\"cs.customer_id\"))\\\n",
    "        .select(\n",
    "            col(\"o.order_id\").cast(\"bigint\"),\n",
    "            col(\"o.order_created_date\").cast(\"date\"),\n",
    "            col(\"o.order_completion_date\").cast(\"date\"),\n",
    "            col(\"o.order_status\"),\n",
    "            col(\"cs.customer_email\"),\n",
    "            col(\"csm.craftsman_email\"),\n",
    "            col(\"o.product_name\")\n",
    "        )\n",
    "    \n",
    "    orders_df_union = union_all(orders_df1, orders_df2, orders_df3).distinct()\n",
    "\n",
    "    # 3. Читаем измерения, которые уже записаны в DWH, для получения сгенерированных идентификаторов.\n",
    "    d_customers = read_table(\"d_customers\", \"dwh\").select(\"customer_id\", \"customer_email\")\n",
    "    d_craftsmans = read_table(\"d_craftsmans\", \"dwh\").select(\"craftsman_id\", \"craftsman_email\")\n",
    "    d_products   = read_table(\"d_products\", \"dwh\").select(\"product_id\", \"product_name\")\n",
    "    \n",
    "    orders_enriched = orders_df_union \\\n",
    "        .join(d_customers, on=\"customer_email\", how=\"left\") \\\n",
    "        .join(d_craftsmans, on=\"craftsman_email\", how=\"left\") \\\n",
    "        .join(d_products, on=\"product_name\", how=\"left\") \\\n",
    "        .withColumn(\"load_dttm\", current_timestamp())\n",
    "    \n",
    "    # 4. Подготовка итогового набора данных для записи в таблицу фактов f_orders\n",
    "    orders_final = orders_enriched.select(\n",
    "        col(\"craftsman_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),             \n",
    "        col(\"order_created_date\"),\n",
    "        col(\"order_completion_date\"),\n",
    "        col(\"order_status\"),\n",
    "        col(\"load_dttm\")\n",
    "    )\n",
    "    \n",
    "    # 5. Запись данных в таблицу фактов f_orders\n",
    "    logger.info(\"Заполнение таблицы фактов f_orders\")\n",
    "    write_table(orders_final, \"f_orders\", \"dwh\", \n",
    "                columns=[\"craftsman_id\", \"customer_id\", \"product_id\", \"order_created_date\", \"order_completion_date\", \"order_status\", \"load_dttm\"])\n",
    "    logger.info(\"ETL-процесс успешно завершён\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Ошибка в ETL-процессе: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    spark.stop()\n",
    "    logger.info(\"SparkSession остановлен\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e765f5e3-64f9-455f-a9fe-29f605cafaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:14:59,306 - INFO - Последняя дата загрузки: 2025-02-19\n",
      "2025-02-19 14:14:59,307 - INFO - Чтение данных из таблицы dwh.d_craftsmans\n",
      "2025-02-19 14:14:59,445 - INFO - Успешно прочитано 17970 строк из dwh.d_craftsmans\n",
      "2025-02-19 14:14:59,446 - INFO - Чтение данных из таблицы dwh.d_customers\n",
      "2025-02-19 14:14:59,570 - INFO - Успешно прочитано 17982 строк из dwh.d_customers\n",
      "2025-02-19 14:14:59,571 - INFO - Чтение данных из таблицы dwh.d_products\n",
      "2025-02-19 14:14:59,693 - INFO - Успешно прочитано 17964 строк из dwh.d_products\n",
      "2025-02-19 14:14:59,694 - INFO - Чтение данных из таблицы dwh.f_orders\n",
      "2025-02-19 14:15:01,327 - INFO - Успешно прочитано 2186478 строк из dwh.f_orders\n",
      "2025-02-19 14:15:01,765 - INFO - Запись данных в таблицу dwh.craftsman_report_datamart\n",
      "2025-02-19 14:15:27,029 - INFO - Данные успешно записаны в dwh.craftsman_report_datamart\n",
      "2025-02-19 14:15:27,165 - INFO - Запись данных в таблицу dwh.load_dates_craftsman_report_datamart\n",
      "2025-02-19 14:15:27,361 - INFO - Данные успешно записаны в dwh.load_dates_craftsman_report_datamart\n",
      "2025-02-19 14:15:27,364 - INFO - Загрузка витрины данных завершена\n",
      "2025-02-19 14:15:27,633 - INFO - SparkSession остановлен\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.functions import current_timestamp, col, date_format, sum, count, avg, percentile_approx, row_number, coalesce, lit, max, first, year, when\n",
    "import logging\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DWH ETL - Data Mart\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/work/postgresql-42.7.1.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Параметры подключения к PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://craft_market_db:5432/craft_market\"\n",
    "connection_properties = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Функция для чтения данных из PostgreSQL\n",
    "def read_table(table_name, schema):\n",
    "    try:\n",
    "        logger.info(f\"Чтение данных из таблицы {schema}.{table_name}\")\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        logger.info(f\"Успешно прочитано {df.count()} строк из {schema}.{table_name}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при чтении таблицы {schema}.{table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Функция для записи данных в PostgreSQL\n",
    "def write_table(df, table_name, schema):\n",
    "    try:\n",
    "        logger.info(f\"Запись данных в таблицу {schema}.{table_name}\")\n",
    "        df.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{schema}.{table_name}\",\n",
    "            mode=\"append\",  # Используем append для витрины\n",
    "            properties=connection_properties\n",
    "        )\n",
    "        logger.info(f\"Данные успешно записаны в {schema}.{table_name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при записи в таблицу {schema}.{table_name}: {str(e)}\")\n",
    "        raise\n",
    "# Загрузка витрины данных\n",
    "def load_datamart():\n",
    "  try:\n",
    "    # 1. Читаем последнюю дату загрузки\n",
    "    try:\n",
    "        last_load_date_df = spark.read.jdbc(url=jdbc_url, table=\"dwh.load_dates_craftsman_report_datamart\", properties=connection_properties)\n",
    "        last_load_date = last_load_date_df.select(max(\"load_dttm\")).first()[0]\n",
    "        logger.info(f\"Последняя дата загрузки: {last_load_date}\")\n",
    "    except Exception as e:\n",
    "        last_load_date = None  # Если таблицы нет или она пуста\n",
    "        logger.info(f\"Таблица с датами загрузки пуста или не существует, загружаем все данные.{e}\")\n",
    "\n",
    "    # 2. Читаем данные из таблиц DWH\n",
    "    d_craftsmans = read_table(\"d_craftsmans\", \"dwh\")\n",
    "    d_customers = read_table(\"d_customers\", \"dwh\")\n",
    "    d_products = read_table(\"d_products\", \"dwh\")\n",
    "    f_orders = read_table(\"f_orders\", \"dwh\")\n",
    "\n",
    "    if last_load_date:\n",
    "        d_craftsmans = d_craftsmans.filter(col(\"load_dttm\") >= last_load_date)\n",
    "        d_customers = d_customers.filter(col(\"load_dttm\") >= last_load_date)\n",
    "        d_products = d_products.filter(col(\"load_dttm\") >= last_load_date)\n",
    "        f_orders = f_orders.filter(col(\"load_dttm\") >= last_load_date)\n",
    "\n",
    "    # 3. Объединяем таблицы\n",
    "    df = f_orders.join(d_craftsmans, \"craftsman_id\") \\\n",
    "                 .join(d_customers, \"customer_id\") \\\n",
    "                 .join(d_products, \"product_id\")\n",
    "\n",
    "    # 4. Вычисляем агрегаты\n",
    "    df = df.withColumn(\"report_period\", date_format(col(\"order_created_date\"), \"yyyy-MM\"))\n",
    "\n",
    "    # 5. Группируем по мастеру и отчетному периоду\n",
    "    df_agg = df.groupBy(\"craftsman_id\", \"report_period\") \\\n",
    "              .agg(\n",
    "                  first(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "                  first(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "                  first(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "                  first(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "                  (sum(col(\"product_price\")) * 0.9).cast(\"decimal(15,2)\").alias(\"craftsman_money\"),\n",
    "                  (sum(col(\"product_price\")) * 0.1).cast(\"bigint\").alias(\"platform_money\"),\n",
    "                  count(\"order_id\").alias(\"count_order\"),\n",
    "                  (avg(col(\"product_price\"))).cast(\"decimal(10,2)\").alias(\"avg_price_order\"),\n",
    "                  avg(year(current_timestamp()) - year(col(\"customer_birthday\"))).cast(\"decimal(3,1)\").alias(\"avg_age_customer\"),\n",
    "                  percentile_approx((col(\"order_completion_date\").cast(\"long\") - col(\"order_created_date\").cast(\"long\")) / (24 * 60 * 60), 0.5).cast(\"decimal(10,1)\").alias(\"median_time_order_completed\"),  # Медиана в днях\n",
    "                  sum(when(col(\"order_status\") == \"created\", 1).otherwise(0)).alias(\"count_order_created\"),\n",
    "                  sum(when(col(\"order_status\") == \"in progress\", 1).otherwise(0)).alias(\"count_order_in_progress\"),\n",
    "                  sum(when(col(\"order_status\") == \"delivery\", 1).otherwise(0)).alias(\"count_order_delivery\"),\n",
    "                  sum(when(col(\"order_status\") == \"done\", 1).otherwise(0)).alias(\"count_order_done\"),\n",
    "                  sum(when(col(\"order_status\").isin([\"created\", \"in progress\", \"delivery\"]), 1).otherwise(0)).alias(\"count_order_not_done\")\n",
    "              )\n",
    "    # Оконная функция для определения самой популярной категории\n",
    "    windowSpec = Window.partitionBy(\"craftsman_id\", \"report_period\").orderBy(col(\"product_count\").desc())\n",
    "\n",
    "    df_with_top_category = df.groupBy(\"craftsman_id\", \"report_period\", \"product_type\") \\\n",
    "        .agg(count(\"product_id\").alias(\"product_count\")) \\\n",
    "        .withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "    top_category = df_with_top_category.filter(col(\"row_number\") == 1).select(col(\"craftsman_id\"),col(\"report_period\"),col(\"product_type\").alias(\"top_product_category\"))\n",
    "\n",
    "    df_agg = df_agg.join(top_category, [\"craftsman_id\", \"report_period\"], \"left\")\n",
    "\n",
    "    # 6. Записываем результат в витрину\n",
    "    write_table(df_agg, \"craftsman_report_datamart\", \"dwh\")\n",
    "\n",
    "    # 7. Обновляем дату последней загрузки\n",
    "    load_date_df = spark.sql(\"select current_date() as load_dttm\")\n",
    "    write_table(load_date_df, \"load_dates_craftsman_report_datamart\", \"dwh\")\n",
    "    logger.info(\"Загрузка витрины данных завершена\")\n",
    "\n",
    "  except Exception as e:\n",
    "        logger.error(f\"Ошибка при загрузке витрины: {str(e)}\")\n",
    "        raise\n",
    "load_datamart()\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession остановлен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925d106-7238-4f64-b354-7cecdabd376a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59acfd-0c43-4752-a1cd-a0ea6634701f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
